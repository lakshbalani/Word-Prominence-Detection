{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Lrss27SHdOuR"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Keras implementation for Deep Embedded Clustering (DEC) algorithm:\n",
        "\n",
        "        Junyuan Xie, Ross Girshick, and Ali Farhadi. Unsupervised deep embedding for clustering analysis. ICML 2016.\n",
        "\n",
        "Usage:\n",
        "    use `python DEC.py -h` for help.\n",
        "\n",
        "Author:\n",
        "    Xifeng Guo. 2017.1.30\n",
        "\"\"\"\n",
        "\n",
        "from time import time\n",
        "import numpy as np\n",
        "import keras.backend as K\n",
        "from keras.layers import Layer, InputSpec\n",
        "from keras.layers import Dense, Input\n",
        "from keras.models import Model\n",
        "from keras.optimizers import SGD\n",
        "from keras import callbacks\n",
        "from keras.initializers import VarianceScaling\n",
        "from sklearn.cluster import KMeans\n",
        "#import metrics\n",
        "import sklearn.metrics as metrics\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "c5LXIOQPdOuV"
      },
      "outputs": [],
      "source": [
        "def autoencoder(dims, act='relu', init='glorot_uniform'):\n",
        "    \"\"\"\n",
        "    Fully connected auto-encoder model, symmetric.\n",
        "    Arguments:\n",
        "        dims: list of number of units in each layer of encoder. dims[0] is input dim, dims[-1] is units in hidden layer.\n",
        "            The decoder is symmetric with encoder. So number of layers of the auto-encoder is 2*len(dims)-1\n",
        "        act: activation, not applied to Input, Hidden and Output layers\n",
        "    return:\n",
        "        (ae_model, encoder_model), Model of autoencoder and model of encoder\n",
        "    \"\"\"\n",
        "    n_stacks = len(dims) - 1\n",
        "    # input\n",
        "    x = Input(shape=(dims[0],), name='input')\n",
        "    h = x\n",
        "\n",
        "    # internal layers in encoder\n",
        "    for i in range(n_stacks-1):\n",
        "        h = Dense(dims[i + 1], activation=act, kernel_initializer=init, name='encoder_%d' % i)(h)\n",
        "\n",
        "    # hidden layer\n",
        "    h = Dense(dims[-1], kernel_initializer=init, name='encoder_%d' % (n_stacks - 1))(h)  # hidden layer, features are extracted from here\n",
        "\n",
        "    y = h\n",
        "    # internal layers in decoder\n",
        "    for i in range(n_stacks-1, 0, -1):\n",
        "        y = Dense(dims[i], activation=act, kernel_initializer=init, name='decoder_%d' % i)(y)\n",
        "\n",
        "    # output\n",
        "    y = Dense(dims[0], kernel_initializer=init, name='decoder_0')(y)\n",
        "\n",
        "    return Model(inputs=x, outputs=y, name='AE'), Model(inputs=x, outputs=h, name='encoder')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "sT2onjnNdOuW"
      },
      "outputs": [],
      "source": [
        "class ClusteringLayer(Layer):\n",
        "    \"\"\"\n",
        "    Clustering layer converts input sample (feature) to soft label, i.e. a vector that represents the probability of the\n",
        "    sample belonging to each cluster. The probability is calculated with student's t-distribution.\n",
        "\n",
        "    # Example\n",
        "    ```\n",
        "        model.add(ClusteringLayer(n_clusters=10))\n",
        "    ```\n",
        "    # Arguments\n",
        "        n_clusters: number of clusters.\n",
        "        weights: list of Numpy array with shape `(n_clusters, n_features)` witch represents the initial cluster centers.\n",
        "        alpha: parameter in Student's t-distribution. Default to 1.0.\n",
        "    # Input shape\n",
        "        2D tensor with shape: `(n_samples, n_features)`.\n",
        "    # Output shape\n",
        "        2D tensor with shape: `(n_samples, n_clusters)`.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_clusters, weights=None, alpha=1.0, **kwargs):\n",
        "        if 'input_shape' not in kwargs and 'input_dim' in kwargs:\n",
        "            kwargs['input_shape'] = (kwargs.pop('input_dim'),)\n",
        "        super(ClusteringLayer, self).__init__(**kwargs)\n",
        "        self.n_clusters = n_clusters\n",
        "        self.alpha = alpha\n",
        "        self.initial_weights = weights\n",
        "        self.input_spec = InputSpec(ndim=2)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        assert len(input_shape) == 2\n",
        "        input_dim = input_shape[1]\n",
        "        self.input_spec = InputSpec(dtype=K.floatx(), shape=(None, input_dim))\n",
        "        self.clusters = self.add_weight(shape=(self.n_clusters, input_dim), initializer='glorot_uniform', name='clusters')\n",
        "        if self.initial_weights is not None:\n",
        "            self.set_weights(self.initial_weights)\n",
        "            del self.initial_weights\n",
        "        self.built = True\n",
        "\n",
        "    def call(self, inputs, **kwargs):\n",
        "        \"\"\" student t-distribution, as same as used in t-SNE algorithm.\n",
        "                 q_ij = 1/(1+dist(x_i, u_j)^2), then normalize it.\n",
        "        Arguments:\n",
        "            inputs: the variable containing data, shape=(n_samples, n_features)\n",
        "        Return:\n",
        "            q: student's t-distribution, or soft labels for each sample. shape=(n_samples, n_clusters)\n",
        "        \"\"\"\n",
        "        q = 1.0 / (1.0 + (K.sum(K.square(K.expand_dims(inputs, axis=1) - self.clusters), axis=2) / self.alpha))\n",
        "        q **= (self.alpha + 1.0) / 2.0\n",
        "        q = K.transpose(K.transpose(q) / K.sum(q, axis=1))\n",
        "        return q\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        assert input_shape and len(input_shape) == 2\n",
        "        return input_shape[0], self.n_clusters\n",
        "\n",
        "    def get_config(self):\n",
        "        config = {'n_clusters': self.n_clusters}\n",
        "        base_config = super(ClusteringLayer, self).get_config()\n",
        "        return dict(list(base_config.items()) + list(config.items()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "hFWUQcs1dOuW"
      },
      "outputs": [],
      "source": [
        "class DEC(object):\n",
        "    def __init__(self,\n",
        "                 dims,\n",
        "                 n_clusters=10,\n",
        "                 alpha=1.0,\n",
        "                 init='glorot_uniform'):\n",
        "\n",
        "        super(DEC, self).__init__()\n",
        "\n",
        "        self.dims = dims\n",
        "        self.input_dim = dims[0]\n",
        "        self.n_stacks = len(self.dims) - 1\n",
        "\n",
        "        self.n_clusters = n_clusters\n",
        "        self.alpha = alpha\n",
        "        self.autoencoder, self.encoder = autoencoder(self.dims, init=init)\n",
        "\n",
        "        # prepare DEC model\n",
        "        clustering_layer = ClusteringLayer(self.n_clusters, name='clustering')(self.encoder.output)\n",
        "        self.model = Model(inputs=self.encoder.input, outputs=clustering_layer)\n",
        "\n",
        "    def pretrain(self, x, y=None, optimizer='adam', epochs=200, batch_size=256, save_dir='results/temp'):\n",
        "        print('...Pretraining...')\n",
        "        self.autoencoder.compile(optimizer=optimizer, loss='mse')\n",
        "\n",
        "        csv_logger = callbacks.CSVLogger(save_dir + '/pretrain_log.csv')\n",
        "        cb = [csv_logger]\n",
        "        if y is not None:\n",
        "            class PrintACC(callbacks.Callback):\n",
        "                def __init__(self, x, y):\n",
        "                    self.x = x\n",
        "                    self.y = y\n",
        "                    super(PrintACC, self).__init__()\n",
        "\n",
        "                def on_epoch_end(self, epoch, logs=None):\n",
        "                    if int(epochs/10) != 0 and epoch % int(epochs/10) != 0:\n",
        "                        return\n",
        "                    feature_model = Model(self.model.input,\n",
        "                                          self.model.get_layer(\n",
        "                                              'encoder_%d' % (int(len(self.model.layers) / 2) - 1)).output)\n",
        "                    features = feature_model.predict(self.x)\n",
        "                    km = KMeans(n_clusters=len(np.unique(self.y)), n_init=20)\n",
        "                    y_pred = km.fit_predict(features)\n",
        "                    # print()\n",
        "                    print(' '*8 + '|==>  acc: %.4f,  nmi: %.4f  <==|'\n",
        "                          % (metrics.accuracy_score(self.y, y_pred), metrics.normalized_mutual_info_score(self.y, y_pred)))\n",
        "\n",
        "            cb.append(PrintACC(x, y))\n",
        "\n",
        "        # begin pretraining\n",
        "        t0 = time()\n",
        "        self.autoencoder.fit(x, x, batch_size=batch_size, epochs=epochs, callbacks=cb)\n",
        "        print('Pretraining time: %ds' % round(time() - t0))\n",
        "        self.autoencoder.save_weights(save_dir + '/ae_weights.h5')\n",
        "        print('Pretrained weights are saved to %s/ae_weights.h5' % save_dir)\n",
        "        self.pretrained = True\n",
        "\n",
        "    def load_weights(self, weights):  # load weights of DEC model\n",
        "        self.model.load_weights(weights)\n",
        "\n",
        "    def extract_features(self, x):\n",
        "        return self.encoder.predict(x)\n",
        "\n",
        "    def predict(self, x):  # predict cluster labels using the output of clustering layer\n",
        "        q = self.model.predict(x, verbose=0)\n",
        "        return q.argmax(1)\n",
        "\n",
        "    @staticmethod\n",
        "    def target_distribution(q):\n",
        "        weight = q ** 2 / q.sum(0)\n",
        "        return (weight.T / weight.sum(1)).T\n",
        "\n",
        "    def compile(self, optimizer='sgd', loss='kld'):\n",
        "        self.model.compile(optimizer=optimizer, loss=loss)\n",
        "\n",
        "    def fit(self, x, y=None, maxiter=2e4, batch_size=256, tol=1e-3,\n",
        "            update_interval=140, save_dir='./results/temp'):\n",
        "\n",
        "        print('Update interval', update_interval)\n",
        "        save_interval = int(x.shape[0] / batch_size) * 5  # 5 epochs\n",
        "        print('Save interval', save_interval)\n",
        "\n",
        "        # Step 1: initialize cluster centers using k-means\n",
        "        t1 = time()\n",
        "        print('Initializing cluster centers with k-means.')\n",
        "        kmeans = KMeans(n_clusters=self.n_clusters, n_init=20)\n",
        "        y_pred = kmeans.fit_predict(self.encoder.predict(x))\n",
        "        y_pred_last = np.copy(y_pred)\n",
        "        self.model.get_layer(name='clustering').set_weights([kmeans.cluster_centers_])\n",
        "\n",
        "        # Step 2: deep clustering\n",
        "        # logging file\n",
        "        import csv\n",
        "        logfile = open(save_dir + '/dec_log.csv', 'w')\n",
        "        logwriter = csv.DictWriter(logfile, fieldnames=['iter', 'acc', 'nmi', 'ari', 'loss'])\n",
        "        logwriter.writeheader()\n",
        "\n",
        "        loss = 0\n",
        "        index = 0\n",
        "        index_array = np.arange(x.shape[0])\n",
        "        for ite in range(int(maxiter)):\n",
        "            if ite % update_interval == 0:\n",
        "                q = self.model.predict(x, verbose=0)\n",
        "                p = self.target_distribution(q)  # update the auxiliary target distribution p\n",
        "\n",
        "                # evaluate the clustering performance\n",
        "                y_pred = q.argmax(1)\n",
        "                if y is not None:\n",
        "                    acc = np.round(metrics.accuracy_score(y, y_pred), 5)\n",
        "                    nmi = np.round(metrics.normalized_mutual_info_score(y, y_pred), 5)\n",
        "                    ari = np.round(metrics.adjusted_rand_score(y, y_pred), 5)\n",
        "                    loss = np.round(loss, 5)\n",
        "                    logdict = dict(iter=ite, acc=acc, nmi=nmi, ari=ari, loss=loss)\n",
        "                    logwriter.writerow(logdict)\n",
        "                    print('Iter %d: acc = %.5f, nmi = %.5f, ari = %.5f' % (ite, acc, nmi, ari), ' ; loss=', loss)\n",
        "\n",
        "                # check stop criterion\n",
        "                delta_label = np.sum(y_pred != y_pred_last).astype(np.float32) / y_pred.shape[0]\n",
        "                y_pred_last = np.copy(y_pred)\n",
        "                if ite > 0 and delta_label < tol:\n",
        "                    print('delta_label ', delta_label, '< tol ', tol)\n",
        "                    print('Reached tolerance threshold. Stopping training.')\n",
        "                    logfile.close()\n",
        "                    break\n",
        "\n",
        "            # train on batch\n",
        "            # if index == 0:\n",
        "            #     np.random.shuffle(index_array)\n",
        "            idx = index_array[index * batch_size: min((index+1) * batch_size, x.shape[0])]\n",
        "            loss = self.model.train_on_batch(x=x[idx], y=p[idx])\n",
        "            index = index + 1 if (index + 1) * batch_size <= x.shape[0] else 0\n",
        "\n",
        "            # save intermediate model\n",
        "            if ite % save_interval == 0:\n",
        "                print('saving model to:', save_dir + '/DEC_model_' + str(ite) + '.h5')\n",
        "                self.model.save_weights(save_dir + '/DEC_model_' + str(ite) + '.h5')\n",
        "\n",
        "            ite += 1\n",
        "\n",
        "        # save the trained model\n",
        "        logfile.close()\n",
        "        print('saving model to:', save_dir + '/DEC_model_final.h5')\n",
        "        self.model.save_weights(save_dir + '/DEC_model_final.h5')\n",
        "\n",
        "        return y_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "YN-CnZM_dOuX"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('feature_vectors.csv')\n",
        "\n",
        "x = df.values.tolist()\n",
        "# convert each element from string to list\n",
        "X = []\n",
        "for i in range(len(x)):\n",
        "    Z = []\n",
        "    for j in range(len(x[i])):\n",
        "        # if x[i][j] is string\n",
        "        if isinstance(x[i][j], str):\n",
        "            p = x[i][j].strip('][').split(', ')\n",
        "            Z.append(p)\n",
        "            # convert each element from string to int\n",
        "            for k in range(len(p)):\n",
        "                p[k] = float(p[k])\n",
        "        else :\n",
        "            break\n",
        "    X.append(Z)\n",
        "X = [item for sublist in X for item in sublist]\n",
        "\n",
        "df2 = pd.read_csv('manual_markings.csv')\n",
        "y = df2.values.tolist()\n",
        "# convert each element from string to list\n",
        "Y = []\n",
        "for i in range(len(y)):\n",
        "    Z = []\n",
        "    for j in range(len(y[i])):\n",
        "        # if x[i][j] is 0 or 1\n",
        "        if y[i][j] == 0 or y[i][j] == 1:\n",
        "            Z.append(int(y[i][j]))\n",
        "        else: break\n",
        "    Y.append(Z)\n",
        "Y = [item for sublist in Y for item in sublist]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "tBAC8U8sdOuX"
      },
      "outputs": [],
      "source": [
        "x = np.array(X)\n",
        "y = np.array(Y)\n",
        "\n",
        "x[np.isnan(x)] = 0\n",
        "y[np.isnan(y)] = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TAO0Pt48dOuY",
        "outputId": "c29f247a-2e1b-4ae8-a390-ad44533a470d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "...Pretraining...\n",
            "Epoch 1/100\n",
            "50/50 [==============================] - 1s 8ms/step\n",
            "        |==>  acc: 0.6766,  nmi: 0.0409  <==|\n",
            "7/7 [==============================] - 5s 444ms/step - loss: 358548.5625\n",
            "Epoch 2/100\n",
            "7/7 [==============================] - 1s 85ms/step - loss: 209992.3281\n",
            "Epoch 3/100\n",
            "7/7 [==============================] - 1s 88ms/step - loss: 51398.6016\n",
            "Epoch 4/100\n",
            "7/7 [==============================] - 1s 92ms/step - loss: 33462.4219\n",
            "Epoch 5/100\n",
            "7/7 [==============================] - 1s 87ms/step - loss: 21180.1660\n",
            "Epoch 6/100\n",
            "7/7 [==============================] - 1s 88ms/step - loss: 6760.5332\n",
            "Epoch 7/100\n",
            "7/7 [==============================] - 1s 87ms/step - loss: 3521.2495\n",
            "Epoch 8/100\n",
            "7/7 [==============================] - 1s 87ms/step - loss: 2530.9958\n",
            "Epoch 9/100\n",
            "7/7 [==============================] - 1s 133ms/step - loss: 2093.0090\n",
            "Epoch 10/100\n",
            "7/7 [==============================] - 1s 89ms/step - loss: 1909.1490\n",
            "Epoch 11/100\n",
            "50/50 [==============================] - 0s 5ms/step\n",
            "        |==>  acc: 0.3240,  nmi: 0.0443  <==|\n",
            "7/7 [==============================] - 1s 228ms/step - loss: 1842.7220\n",
            "Epoch 12/100\n",
            "7/7 [==============================] - 1s 89ms/step - loss: 1754.7581\n",
            "Epoch 13/100\n",
            "7/7 [==============================] - 1s 87ms/step - loss: 1709.1573\n",
            "Epoch 14/100\n",
            "7/7 [==============================] - 1s 88ms/step - loss: 1658.6200\n",
            "Epoch 15/100\n",
            "7/7 [==============================] - 1s 99ms/step - loss: 1580.6578\n",
            "Epoch 16/100\n",
            "7/7 [==============================] - 1s 142ms/step - loss: 1423.0997\n",
            "Epoch 17/100\n",
            "7/7 [==============================] - 1s 144ms/step - loss: 1308.8140\n",
            "Epoch 18/100\n",
            "7/7 [==============================] - 1s 140ms/step - loss: 957.8208\n",
            "Epoch 19/100\n",
            "7/7 [==============================] - 1s 140ms/step - loss: 691.6370\n",
            "Epoch 20/100\n",
            "7/7 [==============================] - 1s 142ms/step - loss: 632.3732\n",
            "Epoch 21/100\n",
            "50/50 [==============================] - 0s 5ms/step\n",
            "        |==>  acc: 0.3240,  nmi: 0.0435  <==|\n",
            "7/7 [==============================] - 2s 362ms/step - loss: 574.3249\n",
            "Epoch 22/100\n",
            "7/7 [==============================] - 1s 85ms/step - loss: 531.2987\n",
            "Epoch 23/100\n",
            "7/7 [==============================] - 1s 89ms/step - loss: 489.4023\n",
            "Epoch 24/100\n",
            "7/7 [==============================] - 1s 87ms/step - loss: 453.1355\n",
            "Epoch 25/100\n",
            "7/7 [==============================] - 1s 88ms/step - loss: 438.2626\n",
            "Epoch 26/100\n",
            "7/7 [==============================] - 1s 85ms/step - loss: 420.9821\n",
            "Epoch 27/100\n",
            "7/7 [==============================] - 1s 88ms/step - loss: 414.6355\n",
            "Epoch 28/100\n",
            "7/7 [==============================] - 1s 98ms/step - loss: 402.1507\n",
            "Epoch 29/100\n",
            "7/7 [==============================] - 1s 130ms/step - loss: 392.1542\n",
            "Epoch 30/100\n",
            "7/7 [==============================] - 1s 166ms/step - loss: 393.7430\n",
            "Epoch 31/100\n",
            "50/50 [==============================] - 0s 5ms/step\n",
            "        |==>  acc: 0.3240,  nmi: 0.0435  <==|\n",
            "7/7 [==============================] - 2s 309ms/step - loss: 398.8897\n",
            "Epoch 32/100\n",
            "7/7 [==============================] - 1s 116ms/step - loss: 401.4761\n",
            "Epoch 33/100\n",
            "7/7 [==============================] - 1s 145ms/step - loss: 391.8229\n",
            "Epoch 34/100\n",
            "7/7 [==============================] - 1s 141ms/step - loss: 386.5769\n",
            "Epoch 35/100\n",
            "7/7 [==============================] - 1s 138ms/step - loss: 387.3112\n",
            "Epoch 36/100\n",
            "7/7 [==============================] - 1s 140ms/step - loss: 390.8438\n",
            "Epoch 37/100\n",
            "7/7 [==============================] - 1s 139ms/step - loss: 371.1197\n",
            "Epoch 38/100\n",
            "7/7 [==============================] - 1s 137ms/step - loss: 365.6473\n",
            "Epoch 39/100\n",
            "7/7 [==============================] - 1s 90ms/step - loss: 366.8705\n",
            "Epoch 40/100\n",
            "7/7 [==============================] - 1s 89ms/step - loss: 371.0568\n",
            "Epoch 41/100\n",
            "50/50 [==============================] - 0s 5ms/step\n",
            "        |==>  acc: 0.3240,  nmi: 0.0435  <==|\n",
            "7/7 [==============================] - 2s 295ms/step - loss: 369.6647\n",
            "Epoch 42/100\n",
            "7/7 [==============================] - 1s 90ms/step - loss: 367.7864\n",
            "Epoch 43/100\n",
            "7/7 [==============================] - 1s 87ms/step - loss: 373.2957\n",
            "Epoch 44/100\n",
            "7/7 [==============================] - 1s 92ms/step - loss: 376.5851\n",
            "Epoch 45/100\n",
            "7/7 [==============================] - 1s 92ms/step - loss: 368.6731\n",
            "Epoch 46/100\n",
            "7/7 [==============================] - 1s 86ms/step - loss: 374.5496\n",
            "Epoch 47/100\n",
            "7/7 [==============================] - 1s 90ms/step - loss: 368.2789\n",
            "Epoch 48/100\n",
            "7/7 [==============================] - 1s 90ms/step - loss: 369.5396\n",
            "Epoch 49/100\n",
            "7/7 [==============================] - 1s 91ms/step - loss: 363.7803\n",
            "Epoch 50/100\n",
            "7/7 [==============================] - 1s 90ms/step - loss: 350.3690\n",
            "Epoch 51/100\n",
            "50/50 [==============================] - 0s 5ms/step\n",
            "        |==>  acc: 0.3246,  nmi: 0.0427  <==|\n",
            "7/7 [==============================] - 3s 412ms/step - loss: 349.4201\n",
            "Epoch 52/100\n",
            "7/7 [==============================] - 1s 143ms/step - loss: 376.7799\n",
            "Epoch 53/100\n",
            "7/7 [==============================] - 1s 139ms/step - loss: 429.4536\n",
            "Epoch 54/100\n",
            "7/7 [==============================] - 1s 144ms/step - loss: 426.4782\n",
            "Epoch 55/100\n",
            "7/7 [==============================] - 1s 139ms/step - loss: 385.1899\n",
            "Epoch 56/100\n",
            "7/7 [==============================] - 1s 98ms/step - loss: 374.2193\n",
            "Epoch 57/100\n",
            "7/7 [==============================] - 1s 86ms/step - loss: 345.2894\n",
            "Epoch 58/100\n",
            "7/7 [==============================] - 1s 88ms/step - loss: 345.8580\n",
            "Epoch 59/100\n",
            "7/7 [==============================] - 1s 85ms/step - loss: 333.3324\n",
            "Epoch 60/100\n",
            "7/7 [==============================] - 1s 91ms/step - loss: 341.5298\n",
            "Epoch 61/100\n",
            "50/50 [==============================] - 0s 5ms/step\n",
            "        |==>  acc: 0.3246,  nmi: 0.0427  <==|\n",
            "7/7 [==============================] - 2s 285ms/step - loss: 340.8519\n",
            "Epoch 62/100\n",
            "7/7 [==============================] - 1s 89ms/step - loss: 378.4472\n",
            "Epoch 63/100\n",
            "7/7 [==============================] - 1s 89ms/step - loss: 333.6925\n",
            "Epoch 64/100\n",
            "7/7 [==============================] - 1s 91ms/step - loss: 325.6581\n",
            "Epoch 65/100\n",
            "7/7 [==============================] - 1s 87ms/step - loss: 320.2436\n",
            "Epoch 66/100\n",
            "7/7 [==============================] - 1s 87ms/step - loss: 315.4142\n",
            "Epoch 67/100\n",
            "7/7 [==============================] - 1s 86ms/step - loss: 321.7136\n",
            "Epoch 68/100\n",
            "7/7 [==============================] - 1s 87ms/step - loss: 322.8160\n",
            "Epoch 69/100\n",
            "7/7 [==============================] - 1s 89ms/step - loss: 314.2110\n",
            "Epoch 70/100\n",
            "7/7 [==============================] - 1s 133ms/step - loss: 285.0627\n",
            "Epoch 71/100\n",
            "50/50 [==============================] - 0s 8ms/step\n",
            "        |==>  acc: 0.3259,  nmi: 0.0408  <==|\n",
            "7/7 [==============================] - 3s 507ms/step - loss: 246.5786\n",
            "Epoch 72/100\n",
            "7/7 [==============================] - 1s 141ms/step - loss: 204.1992\n",
            "Epoch 73/100\n",
            "7/7 [==============================] - 1s 135ms/step - loss: 172.4243\n",
            "Epoch 74/100\n",
            "7/7 [==============================] - 1s 92ms/step - loss: 133.5099\n",
            "Epoch 75/100\n",
            "7/7 [==============================] - 1s 91ms/step - loss: 118.9960\n",
            "Epoch 76/100\n",
            "7/7 [==============================] - 1s 87ms/step - loss: 154.7261\n",
            "Epoch 77/100\n",
            "7/7 [==============================] - 1s 88ms/step - loss: 109.3057\n",
            "Epoch 78/100\n",
            "7/7 [==============================] - 1s 86ms/step - loss: 169.7634\n",
            "Epoch 79/100\n",
            "7/7 [==============================] - 1s 88ms/step - loss: 176.9966\n",
            "Epoch 80/100\n",
            "7/7 [==============================] - 1s 89ms/step - loss: 289.9534\n",
            "Epoch 81/100\n",
            "50/50 [==============================] - 0s 5ms/step\n",
            "        |==>  acc: 0.3240,  nmi: 0.0414  <==|\n",
            "7/7 [==============================] - 2s 359ms/step - loss: 151.0201\n",
            "Epoch 82/100\n",
            "7/7 [==============================] - 1s 85ms/step - loss: 119.7655\n",
            "Epoch 83/100\n",
            "7/7 [==============================] - 1s 87ms/step - loss: 139.2274\n",
            "Epoch 84/100\n",
            "7/7 [==============================] - 1s 91ms/step - loss: 119.4490\n",
            "Epoch 85/100\n",
            "7/7 [==============================] - 1s 85ms/step - loss: 109.3522\n",
            "Epoch 86/100\n",
            "7/7 [==============================] - 1s 90ms/step - loss: 108.7642\n",
            "Epoch 87/100\n",
            "7/7 [==============================] - 1s 139ms/step - loss: 104.5557\n",
            "Epoch 88/100\n",
            "7/7 [==============================] - 1s 143ms/step - loss: 100.5724\n",
            "Epoch 89/100\n",
            "7/7 [==============================] - 1s 142ms/step - loss: 86.6735\n",
            "Epoch 90/100\n",
            "7/7 [==============================] - 1s 144ms/step - loss: 79.2917\n",
            "Epoch 91/100\n",
            "50/50 [==============================] - 0s 7ms/step\n",
            "        |==>  acc: 0.6747,  nmi: 0.0403  <==|\n",
            "7/7 [==============================] - 2s 350ms/step - loss: 87.3722\n",
            "Epoch 92/100\n",
            "7/7 [==============================] - 1s 85ms/step - loss: 82.3938\n",
            "Epoch 93/100\n",
            "7/7 [==============================] - 1s 89ms/step - loss: 79.4120\n",
            "Epoch 94/100\n",
            "7/7 [==============================] - 1s 86ms/step - loss: 86.0632\n",
            "Epoch 95/100\n",
            "7/7 [==============================] - 1s 85ms/step - loss: 85.5862\n",
            "Epoch 96/100\n",
            "7/7 [==============================] - 1s 89ms/step - loss: 77.7774\n",
            "Epoch 97/100\n",
            "7/7 [==============================] - 1s 86ms/step - loss: 71.8438\n",
            "Epoch 98/100\n",
            "7/7 [==============================] - 1s 90ms/step - loss: 68.0460\n",
            "Epoch 99/100\n",
            "7/7 [==============================] - 1s 89ms/step - loss: 69.1734\n",
            "Epoch 100/100\n",
            "7/7 [==============================] - 1s 86ms/step - loss: 65.3549\n",
            "Pretraining time: 92s\n",
            "Pretrained weights are saved to results/ae_weights.h5\n",
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input (InputLayer)          [(None, 15)]              0         \n",
            "                                                                 \n",
            " encoder_0 (Dense)           (None, 500)               8000      \n",
            "                                                                 \n",
            " encoder_1 (Dense)           (None, 500)               250500    \n",
            "                                                                 \n",
            " encoder_2 (Dense)           (None, 2000)              1002000   \n",
            "                                                                 \n",
            " encoder_3 (Dense)           (None, 10)                20010     \n",
            "                                                                 \n",
            " clustering (ClusteringLaye  (None, 2)                 20        \n",
            " r)                                                              \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1280530 (4.88 MB)\n",
            "Trainable params: 1280530 (4.88 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Update interval 10\n",
            "Save interval 30\n",
            "Initializing cluster centers with k-means.\n",
            "50/50 [==============================] - 0s 5ms/step\n",
            "Iter 0: acc = 0.67537, nmi = 0.04064, ari = 0.09857  ; loss= 0\n",
            "saving model to: results/DEC_model_0.h5\n",
            "Iter 10: acc = 0.67473, nmi = 0.04153, ari = 0.09888  ; loss= 0.04334\n",
            "Iter 20: acc = 0.65436, nmi = 0.03460, ari = 0.07912  ; loss= 0.03601\n",
            "Iter 30: acc = 0.65054, nmi = 0.03325, ari = 0.07552  ; loss= 0.04668\n",
            "saving model to: results/DEC_model_30.h5\n",
            "Iter 40: acc = 0.63717, nmi = 0.02904, ari = 0.06370  ; loss= 0.04686\n",
            "Iter 50: acc = 0.62508, nmi = 0.02723, ari = 0.05475  ; loss= 0.05533\n",
            "Iter 60: acc = 0.63399, nmi = 0.02925, ari = 0.06174  ; loss= 0.05494\n",
            "saving model to: results/DEC_model_60.h5\n",
            "Iter 70: acc = 0.61044, nmi = 0.02162, ari = 0.04281  ; loss= 0.04706\n",
            "Iter 80: acc = 0.62763, nmi = 0.02711, ari = 0.05632  ; loss= 0.04849\n",
            "Iter 90: acc = 0.61171, nmi = 0.02205, ari = 0.04377  ; loss= 0.04181\n",
            "saving model to: results/DEC_model_90.h5\n",
            "Iter 100: acc = 0.61680, nmi = 0.02382, ari = 0.04774  ; loss= 0.05053\n",
            "Iter 110: acc = 0.61553, nmi = 0.02337, ari = 0.04673  ; loss= 0.04738\n",
            "Iter 120: acc = 0.60789, nmi = 0.02078, ari = 0.04091  ; loss= 0.05256\n",
            "saving model to: results/DEC_model_120.h5\n",
            "Iter 130: acc = 0.61871, nmi = 0.02380, ari = 0.04889  ; loss= 0.0529\n",
            "Iter 140: acc = 0.60535, nmi = 0.01996, ari = 0.03905  ; loss= 0.04685\n",
            "Iter 150: acc = 0.62126, nmi = 0.02368, ari = 0.05035  ; loss= 0.04898\n",
            "saving model to: results/DEC_model_150.h5\n",
            "Iter 160: acc = 0.60980, nmi = 0.02141, ari = 0.04233  ; loss= 0.04027\n",
            "Iter 170: acc = 0.61680, nmi = 0.02279, ari = 0.04717  ; loss= 0.04871\n",
            "Iter 180: acc = 0.61108, nmi = 0.02117, ari = 0.04293  ; loss= 0.04427\n",
            "saving model to: results/DEC_model_180.h5\n",
            "Iter 190: acc = 0.60917, nmi = 0.02120, ari = 0.04185  ; loss= 0.04923\n",
            "Iter 200: acc = 0.61680, nmi = 0.02279, ari = 0.04717  ; loss= 0.04998\n",
            "Iter 210: acc = 0.60662, nmi = 0.02036, ari = 0.03998  ; loss= 0.04482\n",
            "saving model to: results/DEC_model_210.h5\n",
            "Iter 220: acc = 0.61680, nmi = 0.02178, ari = 0.04658  ; loss= 0.04812\n",
            "Iter 230: acc = 0.60853, nmi = 0.02066, ari = 0.04121  ; loss= 0.03882\n",
            "Iter 240: acc = 0.61489, nmi = 0.02213, ari = 0.04567  ; loss= 0.04699\n",
            "saving model to: results/DEC_model_240.h5\n",
            "Iter 250: acc = 0.60980, nmi = 0.02075, ari = 0.04198  ; loss= 0.04184\n",
            "Iter 260: acc = 0.60853, nmi = 0.02034, ari = 0.04104  ; loss= 0.04707\n",
            "Iter 270: acc = 0.61362, nmi = 0.02137, ari = 0.04450  ; loss= 0.04749\n",
            "saving model to: results/DEC_model_270.h5\n",
            "Iter 280: acc = 0.60471, nmi = 0.01943, ari = 0.03843  ; loss= 0.04247\n",
            "Iter 290: acc = 0.61362, nmi = 0.02039, ari = 0.04392  ; loss= 0.0472\n",
            "Iter 300: acc = 0.60535, nmi = 0.01932, ari = 0.03872  ; loss= 0.03712\n",
            "saving model to: results/DEC_model_300.h5\n",
            "Iter 310: acc = 0.60980, nmi = 0.02011, ari = 0.04162  ; loss= 0.04524\n",
            "Iter 320: acc = 0.60789, nmi = 0.02013, ari = 0.04057  ; loss= 0.03989\n",
            "Iter 330: acc = 0.60598, nmi = 0.01952, ari = 0.03918  ; loss= 0.04525\n",
            "saving model to: results/DEC_model_330.h5\n",
            "Iter 340: acc = 0.60789, nmi = 0.01918, ari = 0.04004  ; loss= 0.04576\n",
            "Iter 350: acc = 0.60153, nmi = 0.01814, ari = 0.03604  ; loss= 0.04062\n",
            "Iter 360: acc = 0.60917, nmi = 0.01864, ari = 0.04040  ; loss= 0.04644\n",
            "saving model to: results/DEC_model_360.h5\n",
            "Iter 370: acc = 0.60216, nmi = 0.01833, ari = 0.03648  ; loss= 0.03571\n",
            "Iter 380: acc = 0.60726, nmi = 0.01836, ari = 0.03921  ; loss= 0.04392\n",
            "Iter 390: acc = 0.60662, nmi = 0.01972, ari = 0.03964  ; loss= 0.03832\n",
            "saving model to: results/DEC_model_390.h5\n",
            "Iter 400: acc = 0.60471, nmi = 0.01912, ari = 0.03827  ; loss= 0.04388\n",
            "Iter 410: acc = 0.60535, nmi = 0.01808, ari = 0.03803  ; loss= 0.04479\n",
            "Iter 420: acc = 0.60025, nmi = 0.01775, ari = 0.03516  ; loss= 0.03939\n",
            "saving model to: results/DEC_model_420.h5\n",
            "Iter 430: acc = 0.60662, nmi = 0.01786, ari = 0.03857  ; loss= 0.04597\n",
            "Iter 440: acc = 0.60025, nmi = 0.01775, ari = 0.03516  ; loss= 0.03488\n",
            "Iter 450: acc = 0.60407, nmi = 0.01709, ari = 0.03678  ; loss= 0.04309\n",
            "saving model to: results/DEC_model_450.h5\n",
            "Iter 460: acc = 0.60089, nmi = 0.01764, ari = 0.03544  ; loss= 0.03706\n",
            "Iter 470: acc = 0.60025, nmi = 0.01745, ari = 0.03500  ; loss= 0.04299\n",
            "delta_label  0.0006365372374283895 < tol  0.001\n",
            "Reached tolerance threshold. Stopping training.\n",
            "saving model to: results/DEC_model_final.h5\n"
          ]
        }
      ],
      "source": [
        "n_clusters = len(np.unique(y))\n",
        "\n",
        "init = 'glorot_uniform'\n",
        "pretrain_optimizer = 'adam'\n",
        "\n",
        "save_dir = 'results'\n",
        "if not os.path.exists(save_dir):\n",
        "    os.makedirs(save_dir)\n",
        "\n",
        "# prepare the DEC model\n",
        "dec = DEC(dims=[x.shape[-1], 500, 500, 2000, 10], n_clusters=n_clusters, init=init)\n",
        "\n",
        "dec.pretrain(x=x, y=y, optimizer=pretrain_optimizer,\n",
        "                epochs=100, batch_size=256,\n",
        "                save_dir=save_dir)\n",
        "\n",
        "dec.model.summary()\n",
        "t0 = time()\n",
        "dec.compile(optimizer=SGD(0.02, 0.9), loss='kld')\n",
        "y_pred = dec.fit(x, y=y, tol=0.001, maxiter=2e4, batch_size=256,\n",
        "                    update_interval=10, save_dir=save_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1j0qlyzHfQzd",
        "outputId": "0a2e3e75-edb8-41fc-fae4-fbb018d17754"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "acc : 0.6429853596435392\n",
            "clustering time : 68.45387244224548\n"
          ]
        }
      ],
      "source": [
        "print('acc :', metrics.accuracy_score(y, y_pred))\n",
        "print('clustering time :', (time() - t0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "O178Fhk4dOuY"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('indian_accent_feature_vectors.csv')\n",
        "\n",
        "x_indian = df.values.tolist()\n",
        "# convert each element from string to list\n",
        "X_indian = []\n",
        "for i in range(len(x_indian)):\n",
        "    Z = []\n",
        "    for j in range(len(x_indian[i])):\n",
        "        # if x_indian[i][j] is string\n",
        "        if isinstance(x_indian[i][j], str):\n",
        "            p = x_indian[i][j].strip('][').split(', ')\n",
        "            Z.append(p)\n",
        "            # convert each element from string to int\n",
        "            for k in range(len(p)):\n",
        "                p[k] = float(p[k])\n",
        "        else :\n",
        "            break\n",
        "    X_indian.append(Z)\n",
        "X_indian = [item for sublist in X_indian for item in sublist]\n",
        "\n",
        "df2 = pd.read_csv('word_markings_indian_accent.csv')\n",
        "y_indian = df2.values.tolist()\n",
        "# convert each element from string to list\n",
        "Y_indian = []\n",
        "for i in range(len(y_indian)):\n",
        "    Z = []\n",
        "    for j in range(len(y_indian[i])):\n",
        "        # if x[i][j] is 0 or 1\n",
        "        if y_indian[i][j] == 0 or y_indian[i][j] == 1:\n",
        "            Z.append(int(y_indian[i][j]))\n",
        "        else: break\n",
        "    Y_indian.append(Z)\n",
        "Y_indian = [item for sublist in Y_indian for item in sublist]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "KOFJCx6DezOp"
      },
      "outputs": [],
      "source": [
        "x_indian = np.array(X_indian)\n",
        "y_indian = np.array(Y_indian)\n",
        "\n",
        "x_indian[np.isnan(x_indian)] = 0\n",
        "y_indian[np.isnan(y_indian)] = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d7ZfGnh1ettx"
      },
      "outputs": [],
      "source": [
        "y_pred = dec.fit(x_indian, y=y_indian, tol=0.001, maxiter=2e4, batch_size=256,\n",
        "                    update_interval=10, save_dir=save_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Ye23Rt1e9eH",
        "outputId": "a683410b-6711-416d-af7e-c857e28e898a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "acc : 0.5726719004126904\n",
            "clustering time : 98.61964917182922\n"
          ]
        }
      ],
      "source": [
        "print('acc :', metrics.accuracy_score(y_indian, y_pred))\n",
        "print('clustering time :', (time() - t0))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nWInR8vnclDy"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
